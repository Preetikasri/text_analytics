{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "668d7b64-6744-4d20-b0af-7dca224cc731"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from pandas import Series, DataFrame\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import operator\n",
    "import re,string\n",
    "from patsy import dmatrices\n",
    "%pylab inline\n",
    "from nltk.corpus import stopwords\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import random\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "701acea8-37d8-448d-8a20-3bc8ae2dc9aa"
    }
   },
   "source": [
    "### A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "54a4e958-97e5-4d5e-94d7-d1e9dbfca82e"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "38557025-bf04-4668-b312-5ae57686a239"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "3d6adfcd-7d7a-407f-b96e-1783e1e68c1e"
    }
   },
   "source": [
    "In order to find the top 5 POS, we need to first tokenize the full description.<br> \n",
    "We'll do this by first taking a random sample size of 2500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "f536e3d4-9935-4c57-97ff-2dbe34521ae7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105900    Passionate about making lives better, Bupa is ...\n",
       "99812     Category Manager  Milton Keynes High profile r...\n",
       "52448     The Company: Our client enjoys a high profile ...\n",
       "Name: FullDescription, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(99)\n",
    "sample_2500 = random.sample(range(len(train_data)),2500)\n",
    "sample_data = train_data['FullDescription'][sample_2500]\n",
    "sample_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a92f8294-c9be-4d96-9454-0ccd7035042a"
    }
   },
   "source": [
    "Then we conduct nltk tokenize to our sample. But there's still some steps before tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "dce7457c-e643-48a9-ac22-e03f09c5edb7"
    }
   },
   "outputs": [],
   "source": [
    "full_des = sample_data.apply(lambda x:re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", x.lower())).sum()\n",
    "des_words = nltk.word_tokenize(full_des)\n",
    "des_words = [word for word in des_words if word.isalpha()==True] #get rid of punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "809c7cb5-6964-4a05-952f-a959e815b05c"
    }
   },
   "source": [
    "Then, get the top 5 POS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "90b3d01f-a2e7-42f5-8cf6-83b0a950d218"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cc32f8c09293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pos' is not defined"
     ]
    }
   ],
   "source": [
    "cnt = Counter(tag for word,tag in pos)\n",
    "cnt.most_common()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1ac98cee-a2a1-4f44-9f2b-8bee92879f1d"
    }
   },
   "source": [
    "Looks like 'NN': Singular Noun, 'JJ': Adjective, 'IN': Preposition or subordinating conjunction, 'NNS': Pural Noun and 'DT': Determiner are the 5 most common POS in the description.<br>\n",
    "<br>\n",
    "Let's do the process again and this time, exclude the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a41c2848-e9d9-4287-971a-1f23753b634e"
    }
   },
   "outputs": [],
   "source": [
    "#This step takes a *really* long time to run since the perceptor has to load everytime.\n",
    "stop = set(stopwords.words('english'))\n",
    "filtered_stopwords = [word for word in des_words if word not in stop]\n",
    "filtered_pos = nltk.pos_tag(filtered_stopwords)\n",
    "cnt2 = Counter(tag for word,tag in filtered_pos)\n",
    "cnt2.most_common()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d65a7e6a-31ec-4b99-ac9a-a245daf3c03f"
    }
   },
   "source": [
    "After excluding the stopwords, Singular Noun, Adjective and Pural Noun are still in the top 5 list. Verb(gerund or present participle) and Verb(non-3rd person singular present) are added in the top 5 POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "36e0fa9b-b0b8-4b81-8fce-f598c4285ba3"
    }
   },
   "source": [
    "### A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, calculate the frequency of words and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9de0ae55-eb61-423a-82fa-2843a91ad708"
    }
   },
   "outputs": [],
   "source": [
    "#Calculate frequency.\n",
    "fdist = nltk.FreqDist(des_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Zipf's law for top 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the top 100 against Zipf's Law\n",
    "plt.figure(figsize=(20,10))\n",
    "fdist.plot(100, cumulative=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "468f8120-cc74-472d-b15e-ed574dccc48a"
    }
   },
   "outputs": [],
   "source": [
    "#Sorted by frequency\n",
    "sort_fdist = pd.DataFrame(sorted(fdist.items(), key=operator.itemgetter(1),reverse=True))\n",
    "most_common_100 = sort_fdist[:100]\n",
    "most_common_100.columns = ['word','frequency']\n",
    "most_common_100['rank'] = most_common_100['frequency'].rank(method='min',ascending=False)\n",
    "most_common_100['zipf_law'] = [most_common_100[\"frequency\"].max()/r for r in most_common_100['rank']]\n",
    "most_common_100[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "x = [math.log(c) for c in most_common_100['rank'].values]\n",
    "y1 = [math.log(c) for c in most_common_100['frequency']]\n",
    "y2 = [math.log(c) for c in most_common_100['zipf_law']]\n",
    "\n",
    "ax1 = plt.plot(x,y1,label='Actual')\n",
    "ax2 = plt.plot(x,y2,label='Theorical')\n",
    "\n",
    "xlabel(\"log(rank)\")\n",
    "ylabel(\"log(frequency)\")\n",
    "title('Top 100')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results show that the top 100 most does follow the Zipf's law in generally.<br>\n",
    "Now, let's test the Zipf's law with the entire sample (2500 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import datasets, linear_model\n",
    "most_common_100['Y'] = [math.log(c) for c in most_common_100['frequency']]\n",
    "x = most_common_100['frequency'] / (most_common_100['frequency'].max() * most_common_100.shape[0])\n",
    "most_common_100['X'] = [math.log(c) for c in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('Y ~ 0 + X', data=most_common_100, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)       # Set up the model\n",
    "#model = linear_model.LinearRegression()\n",
    "result = model.fit()       # Fit model (find the intercept and slopes)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the coeff is -0.9927, which is close to -1. Hence we can assume that the top 100 words in job description follow Zipf's law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Zipf's law for entire sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_fdist.columns = ['word','frequency']\n",
    "sort_fdist['rank'] = sort_fdist['frequency'].rank(method='min',ascending=False)\n",
    "sort_fdist['zipf_law'] = [sort_fdist[\"frequency\"].max()/r for r in sort_fdist['rank']]\n",
    "sort_fdist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "x = [math.log(c) for c in sort_fdist['rank'].values]\n",
    "y1 = [math.log(c) for c in sort_fdist['frequency']]\n",
    "y2 = [math.log(c) for c in sort_fdist['zipf_law']]\n",
    "\n",
    "ax1 = plt.plot(x,y1,label='Actual')\n",
    "ax2 = plt.plot(x,y2,label='Theorical')\n",
    "\n",
    "xlabel(\"log(rank)\")\n",
    "ylabel(\"log(frequency)\")\n",
    "title('Entire sample')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Zipf's law empirically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import datasets, linear_model\n",
    "sort_fdist['Y'] = [math.log(c) for c in sort_fdist['frequency']]\n",
    "x = sort_fdist['frequency'] / (sort_fdist['frequency'].max() * sort_fdist.shape[0])\n",
    "sort_fdist['X'] = [math.log(c) for c in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('Y ~ 0 + X', data=sort_fdist, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.OLS(y, X)       # Set up the model\n",
    "#model = linear_model.LinearRegression()\n",
    "result = model.fit()       # Fit model (find the intercept and slopes)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coeff is negative, however closer to 0. Hence we can conclude that in the sample of 2500, the words occurring are not representative of any power law, in this case Zipf's law. This can be attributed to the fact that the words with lower ranks do not behave in this fashion. We will test for the same in the following pieces of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Zipf's law for last 15000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = dmatrices('Y ~ 0 + X', data=sort_fdist.tail(15000), return_type='dataframe')\n",
    "model = sm.OLS(y, X)       # Set up the model\n",
    "#model = linear_model.LinearRegression()\n",
    "result = model.fit()       # Fit model (find the intercept and slopes)\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see here, that these words do not follow the power law.\n",
    "\n",
    "To summarize, as a whole the data doesn't support Zipf's Law. However, Zipf's Law will work up to a certain frequency. In this case, we tested it empirically for up to 100 words and observed that those 100 words belonged to a specific distribution of Zipf's Law. We also tested for the last 15000 words and observed that they don't belong to the Zipf's distribution and those points might be driving the sample to not follow the power law. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "54c84700-6524-4b68-922f-0edcd6725ea7"
    }
   },
   "source": [
    "### A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c60725e6-c951-49d7-8a87-d378bd6d6a81"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "#create a function that would return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "         return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "#create an empty list to store lemmatized words\n",
    "des_lem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_pos(filtered_pos):\n",
    "    for word,pos in filtered_pos:\n",
    "        des_lem.append(wnl.lemmatize(word,get_wordnet_pos(pos)))\n",
    "        #print pos\n",
    "        #print get_wordnet_pos(pos)\n",
    "    return des_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 10 most common words\n",
    "fdist_2 = nltk.FreqDist(wn_pos(filtered_pos))\n",
    "fdist_2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10 most common words that appears in the job descriptions are shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1 & B2\n",
    "#### Model with numeric columns only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Reading training data\n",
    "data = pd.read_csv('Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = data[['LocationNormalized','ContractType','ContractTime','Category','SalaryNormalized']]\n",
    "print(data_s.shape)\n",
    "data_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking NA in Contract Type\n",
    "data_s.ContractType.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ~73% of ContractType is missing, we will not be using this column for our classification. Replacing NaN with \"Full Time\" will bias the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = data[['LocationNormalized','ContractTime','Category','SalaryNormalized']]\n",
    "print(data_s.shape)\n",
    "df = data_s.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By dropping all rows with missing values, we lost about 65K rows (~26%). We will be using the clean dataset going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.percentile(df['SalaryNormalized'],75)\n",
    "def target(t):\n",
    "    if t>p:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "df['target'] = df['SalaryNormalized'].map(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of cities with highest cost of living."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the top 10 highest CoL data from https://abcfinance.co.uk/blog/the-true-cost-of-living-in-uk-cities/\n",
    "high_cost = ['London','Milton Keynes','Bath','Reading','Aberdeen','Cambridge','Oxford','Portsmouth','Edinburgh','York']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_class(s):\n",
    "    if s in high_cost:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'low'\n",
    "df['location_class'] = df['LocationNormalized'].map(location_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the dummies for each variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['ContractTime', 'Category','location_class']\n",
    "data_dummies = pd.get_dummies(df[categorical_columns],\n",
    "                            prefix=categorical_columns,\n",
    "                            columns=categorical_columns)\n",
    "dummy_column_names = data_dummies.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df, data_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build Naive Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'target ~ 0 + {}'.format(' + '.join(['Q(\"{}\")'.format(x) for x in dummy_column_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, X = dmatrices(formula, df2, return_type='dataframe')\n",
    "y = Y['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is no validation dataset, we will create a testing/training sample here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "model = naive_bayes.BernoulliNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking training accuracy\n",
    "from sklearn import metrics\n",
    "prediction_train = model.predict(X_test)\n",
    "print(metrics.accuracy_score(y_test, prediction_train))\n",
    "print(\"Test data confusion matrix\")\n",
    "confusion_matrix(y_test, prediction_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The accuracy of using numerical variables are 76.06% on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, prediction_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with text columns only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the dataset for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_s = data[['FullDescription','SalaryNormalized']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.percentile(data_s['SalaryNormalized'],75)\n",
    "def target(t):\n",
    "    if t>p:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'low'\n",
    "    \n",
    "data_s['target'] = data_s['SalaryNormalized'].map(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a sample of 2500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(99)\n",
    "#sample = random.sample(range(len(data_s)),0.7 * len(data_s))\n",
    "sample = random.sample(range(len(data_s)),2500)\n",
    "df = data_s.loc[sample,:]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the job description by:\n",
    "1. Removing punctuation\n",
    "2. Getting rid of stop words\n",
    "3. Removing Numbers\n",
    "4. Stripping excess whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "#removing punctuation and numbers\n",
    "df['job_des'] = df['FullDescription'].apply(lambda x:re.sub(r'[^a-zA-z\\s]', ' ', x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove white spaces\n",
    "df['job_des'] = df.job_des.apply(lambda x:re.sub(r'\\s+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "df['job_des_clean'] = df.job_des.apply(lambda x: [word for word in x.split() if word not in stop])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating training data for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using just first 2000 words as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_des_all = df['job_des_clean'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(job_des_all)\n",
    "word_features = list(all_words)[:2000]\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[['target','job_des_clean']]\n",
    "t = list(zip(df2.melt('target').value,df2.melt('job_des_clean').value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(document_features(x[0]), x[1]) for x in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = featuresets[2000:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The accuracy of the text only model is 79.6%. We used Binomial Naive Bayes classification for both text and numeric models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The top 10 words indicating high salary and low salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Bag of words to convert the text data to numeric data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nl\n",
    "import os\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name = 'job_salary'\n",
    "# zf = zipfile.ZipFile('../data/'+dataset_name+ '.zip')\n",
    "# files = zf.infolist()\n",
    "\n",
    "# for f in files:\n",
    "#     print(\"file present here is\", f.filename)\n",
    "#     df = pd.read_csv(zf.open(f.filename))\n",
    "#if the data was in zipfiles, we would read it with the code above^\n",
    "df = pd.read_csv('Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chosen = df.sample(2500, random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['location','contract_type','contract_time','type_of_job','salary']\n",
    "num_data = df_chosen[['LocationNormalized','ContractType','ContractTime','Category','SalaryNormalized']]\n",
    "num_data.columns = col_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.percentile(num_data['salary'],75)\n",
    "def target(t):\n",
    "    if t>p:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "num_data['target'] = num_data['salary'].map(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cost = ['London','Milton Keynes','Bath','Reading','Aberdeen','Cambridge','Oxford','Portsmouth','Edinburgh','York']\n",
    "def location_class(s):\n",
    "    if s in high_cost:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "num_data['location_class'] = num_data['location'].map(location_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.pop('location')\n",
    "num_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data[['contract_type', 'contract_time','type_of_job']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since, many of the observations in 'contract_time', 'contract_type' are null, let's impute them using the most frequent value for  them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data['contract_time'] = num_data['contract_time'].fillna('permanent')\n",
    "num_data['contract_type'] = num_data['contract_type'].fillna('full_time')\n",
    "num_data[['contract_type', 'contract_time','type_of_job']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = num_data[['contract_type', 'contract_time', 'type_of_job','location_class','target']]\n",
    "categorical_columns = ['contract_type', 'contract_time', 'type_of_job']\n",
    "#num_data['contract_type'] = num_data['contract_time'].astype('category')\n",
    "df_model = pd.get_dummies(df_model, columns= categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.shape   ## Numerical data tranformed to dummy variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we include Text data, and make a boolean vector for the corpus' vocabulary\n",
    "### Using a Bag o' Words approach to transform text to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = list(df_chosen['FullDescription'])\n",
    "vectorizer = CountVectorizer()\n",
    "corpus_bool = vectorizer.fit_transform(corpus).todense() \n",
    "corpus_bool[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bool.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame(vectorizer.fit_transform(corpus).todense(), columns= vectorizer.vocabulary_ )\n",
    "dict_ = vectorizer.vocabulary_\n",
    "dict_sorted = sorted(dict_ , key= lambda x: dict_[x], reverse=True) ## Sorting the dictionary based on frequency of words\n",
    "dict_sorted = dict_sorted[:3000] ## Retaining only 3000 words\n",
    "dictfilt = lambda x, y: dict([ (i,x[i]) for i in x if i in set(y) ])\n",
    "dict_filtrered = dictfilt(dict_, dict_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(dictfilt(dict_, dict_sorted).keys())\n",
    "word_df = word_df[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_df.shape) \n",
    "#word_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.rename(columns={'target':'my_response_variable'}, inplace=True)\n",
    "df_model = df_model.reset_index(drop=True)\n",
    "word_df= word_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = df_model.join(word_df)\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_target = full_data['my_response_variable']\n",
    "full_data.drop('my_response_variable', inplace=True, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(full_data, Y_target, test_size=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## User Defined Function for model training\n",
    "def fit_the_model(X_train, y_train, model_name):\n",
    "    if(model_name == 'NB'):\n",
    "        clf = BernoulliNB()\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\" Model fitting done by Naive Bayes Bernoulli\")\n",
    "        \n",
    "    elif(model_name == 'RF'):\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        print(\"Model fitting done by Random Forest\")\n",
    "        \n",
    "    return clf\n",
    "## User Defined Function for model evaluation\n",
    "\n",
    "def classification_model_evaluation(model, X_train, y_train, X_test, y_test):\n",
    "    prediction_train = model.predict(X_train)\n",
    "    prediction_test = model.predict(X_test)\n",
    "    print (\"Accuracy on Training data is\", metrics.accuracy_score(y_train, prediction_train))\n",
    "    print (\"Accuracy on Test data is\", metrics.accuracy_score(y_test, prediction_test))\n",
    "    print('\\n')\n",
    "    print(\"Confusion Matrix FOR TEST Obtained is: \\n \", confusion_matrix(y_test, prediction_test))\n",
    "    print('\\n')\n",
    "    print(\"Confusion Matrix FOR TRAIN Obtained is: \\n \", confusion_matrix(y_train, prediction_train))\n",
    "    report = classification_report(y_test, prediction_test)\n",
    "    print('\\n')\n",
    "    print(\"Classification Report \\n\", report)\n",
    "    return (\"Printed All the metrics for your classification model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mod = fit_the_model(X_train, y_train, 'NB')\n",
    "classification_model_evaluation(my_mod,X_train, y_train, X_test, y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rf_mod = fit_the_model(X_train, y_train, 'RF')\n",
    "classification_model_evaluation(my_rf_mod,X_train, y_train, X_test, y_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It is evident that Naive Bayes performs better , as it doesn't result in overfittng, unlike Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which model – numeric only, text only and hybrid – provided the highest accuracy in predicting high/low salary? Did the result surprise you? Why or why not?\n",
    "\n",
    "| Model               | Test Accuracy     |\n",
    "|---------------      |---------------    |\n",
    "| Numeric only        |      76%          |\n",
    "| Text only           |     79.6%         |\n",
    "| Hybrid Naive Bayes  |     77.6%         |\n",
    "| Hybrid Random Forest|     76.8%         |\n",
    "\n",
    "\n",
    "The results did surprise us, because we assumed that the hybrid, which is made up of numbers and text, should be more accurate because of more data. However, the text only model was the most accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
